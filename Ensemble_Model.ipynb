{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Ensemble Method </h4>\n",
    "Each review is mapped to a feature vector by averaging the word embeddings of all words in the review. These features are then fed into a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'description', 'two_outcomes'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('meunier', 0.8507343530654907),\n",
       " ('noirs', 0.7709870934486389),\n",
       " ('gris', 0.7536669969558716),\n",
       " ('bianco', 0.7067205309867859),\n",
       " ('grigio', 0.6630685329437256),\n",
       " ('nero', 0.5673935413360596),\n",
       " ('silkiness', 0.525829553604126),\n",
       " ('oregon', 0.4896107017993927),\n",
       " ('immature', 0.4749774932861328),\n",
       " ('california', 0.4512163996696472)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = os.path(r\"Module 4/Supervised NN Project/train_model\")\n",
    "%store -r\n",
    "reviews = pd.read_csv('_review_data.csv')\n",
    "print(reviews.columns)\n",
    "model.wv.most_similar(\"noir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nice', 0.831934928894043),\n",
       " ('decent', 0.7923933863639832),\n",
       " ('proper', 0.7887036204338074),\n",
       " ('great', 0.7273436784744263),\n",
       " ('fine', 0.7250930666923523),\n",
       " ('excellent', 0.7233678102493286),\n",
       " ('adequate', 0.6736984848976135),\n",
       " ('solid', 0.660475492477417),\n",
       " ('fair', 0.5983995199203491),\n",
       " ('enough', 0.5882049798965454)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = model.wv\n",
    "w2v.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(w2v['nice'].shape)\n",
    "num_features = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2word_set = set(w2v.index2word)\n",
    "# len(index2word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \n",
    "#     Average the word vectors for a set of words\n",
    "    \n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(w2v.index2word)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    \n",
    "#     Calculate average feature vectors for all wine reviews\n",
    "    \n",
    "    counter = 0\n",
    "    desc_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')\n",
    "    \n",
    "    for desc in reviews:\n",
    "        desc_feature_vecs[counter] = make_feature_vec(desc, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return desc_feature_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec of description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90934.9\n"
     ]
    }
   ],
   "source": [
    "print((reviews.shape[0] * 0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129907 129907\n"
     ]
    }
   ],
   "source": [
    "train_reviews = reviews[:90935]\n",
    "test_reviews = reviews[90935:]\n",
    "print(reviews.shape[0], len(train_reviews) + len(test_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculate average feature vectors for training and test sets\n",
    "\n",
    "trainDataVecs = get_avg_feature_vecs(train_reviews['description'], model, num_features)\n",
    "\n",
    "testDataVecs = get_avg_feature_vecs(test_reviews['description'], model, num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08443289, -0.01748779, -0.18085916, -0.14912176,  0.00550791,\n",
       "       -0.05918663, -0.05578178, -0.16033605,  0.07697774, -0.0874128 ,\n",
       "       -0.06674305, -0.17707986, -0.06660268,  0.20295903, -0.12143666,\n",
       "       -0.19695324,  0.09940048, -0.14379378, -0.17519991, -0.07587919,\n",
       "        0.01958998,  0.06260195, -0.13870066, -0.11189912,  0.02611933,\n",
       "        0.05223307,  0.13144659, -0.05290572,  0.08743954, -0.11106408,\n",
       "       -0.14252013,  0.27590382,  0.05521486, -0.16753085,  0.17436348,\n",
       "        0.01557619,  0.0399346 , -0.03053275,  0.1015004 , -0.24072695,\n",
       "        0.10247923, -0.08319446,  0.01942956, -0.17561089, -0.10253713,\n",
       "        0.15378103, -0.0026733 , -0.1250518 , -0.16158238,  0.08599287],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit a random forest to the training data, using 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainDataVecs, train_reviews['two_outcomes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove instances in test set that could not be represented as feature vectors\n",
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(testDataVecs))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from test set.'.format(len(nan_indices)))\n",
    "    testDataVecs = np.delete(testDataVecs, nan_indices, axis=0)\n",
    "    test_reviews.drop(test_reviews.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert testDataVecs.shape[0] == len(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for test data..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0d49f545538e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicting labels for test data..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDataVecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'two_outcomes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'forest' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Predicting labels for test data..\")\n",
    "result = forest.predict(testDataVecs)\n",
    "print(classification_report(test_reviews['two_outcomes'], result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = forest.predict_proba(testDataVecs)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_reviews['two_outcomes'], probs)\n",
    "auc = roc_auc_score(test_reviews['two_outcomes'], probs)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='AUC {:.3f}'.format(auc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "plt.savefig('AUC ROC Wine Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store trainDataVecs\n",
    "%store testDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_reviews\n",
    "%store test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at confusion matrix of results\n",
    "y_hat_train = model_three.predict(X_nn_train)\n",
    "y_hat_test = model_three.predict(X_nn_test)\n",
    "\n",
    "classes = [0, 1]\n",
    "\n",
    "cm = confusion_matrix(labels_train, np.round_(y_hat_train, decimals=0))\n",
    "plot_conf_matrix(cm, classes )\n",
    "plt.savefig('Confusion Matrix')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
